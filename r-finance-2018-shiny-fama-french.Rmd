---
title: "R Finance 2018 Notebook"
output: html_notebook
---


```{r setup, include = FALSE}
 
library(tidyquant)
library(tidyverse)
library(timetk)
library(broom)
library(tibbletime)
library(highcharter)
library(scales)
library(riingo)

knitr::opts_chunk$set(message=FALSE, warning=FALSE)
```

In this presentation, we will build a Shiny application that allows a user to construct a portfolio, choose a set of Fama French factors, regress the portfolio returns on the Fama French factors and visualize the results.

The final app is viewable here: 

www.reproduciblefinance.com/shiny/fama-french-choose-factors/

The model and regression results are probably not new to the audience but we investigate some tools with broad applicability but the app is a reproducible template for building a custom portfolio, importing another data set, running a rolling multilinear regression and visualizing the results. It is purpose built for Fama French but can be generalized to any two data sets that need to be wrangled together, then modeled, then visualized.


###  Get Daily Returns and Build Portfolio

We want to give the user the ability to build a custom portfolio and we will import daily prices from a relatively new source, [tiingo](tiingo.com), making use of the `riingo` package. I like tiingo as a new source for price data and fundamental data may be in the offing. I'm also tired of ya**o! finance.

We'll choose five stock tickers.
```{r}
# The symbols vector holds our tickers. 
symbols <- c("SPY","EFA", "IJS", "EEM","AGG")
```

Then enter our tiingo API key.

```{r}

# Need an API key for tiingo

riingo_set_token("your api key here")
```

And now use the `riingo_prices()` function to get our daily prices.

```{r}
# Get daily prices.
prices_riingo <- 
  riingo_prices(symbols, start_date = "2017-01-01") %>% 
  select(ticker, date, adjClose)
```

We convert to log returns using `mutate()`.

```{r}

# Convert to returns
returns_riingo <- 
  prices_riingo %>%
  group_by(ticker) %>%  
  mutate(returns = (log(adjClose) - log(lag(adjClose)))) 
```

To create a portfolio, we need weights.

```{r}
# Create a vector of weights  
w <- c(0.25, 0.25, 0.20, 0.20, 0.10)
```

And now we pass the individual returns and weights to dplyr.

```{r}
# Create a portfolio and calculate returns  
portfolio_riingo <-  
  returns_riingo %>% 
  mutate(weights = case_when(
    ticker == symbols[1] ~ w[1],
    ticker == symbols[2] ~ w[2],
    ticker == symbols[3] ~ w[3],
    ticker == symbols[4] ~ w[4],
    ticker == symbols[5] ~ w[5]), 
  weighted_returns = returns * weights) %>% 
  group_by(date) %>% 
  summarise(returns = sum(weighted_returns))

```

We could also use `tq_portfolio()`.

```{r}
portfolio_returns_tq <- 
  returns_riingo %>%
  tq_portfolio(assets_col  = ticker, 
               returns_col = returns,
               weights     = w,
               col_rename  = "returns")
```


Importing daily prices and converting to portfolio returns is not a complex job, but it's still good practice to detail the steps for when our future self or a colleague wishes to revisit this work in 6 months and use it as a foundation for involved work. We can also see how this code flow gets ported almost directly over to our Shiny application.

### Importing and Wrangling the Fama French Factors

We need to get the Fama French factors data, which is not available on yahoo! Finance. Luckily Fama and French make their factor data available on their website.  We are going to document each step for importing and cleaning this data, to an extent that might be overkill. It can be a grind to document these steps now, but a time saver later when we need to update our Shiny app.  If someone else needs to update the model in the future, detailed data import steps are crucial.

Have a look at the website where factor data is available.

http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html

The data are packaged as zip files so we'll need to do a bit more than call `read_csv()`.  

We will use the `tempfile()` function from base R to create a variable called `temp`, and will store the zipped file there.

Now we invoke `downloadfile()`, pass it the URL address of the zip, which for daily Global 5 Factors  is "http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/Global_5_Factors_Daily_CSV.zip".

However, I choose not to pass that URL in directly, instead I paste it together in pieces with

`factors_input <- "Global_5_Factors_Daily"`

`factors_address <- paste("http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/", factors_input, "_CSV.zip", sep="" )`

The reason for that is eventually we want to give the user the ability to choose different factors in the Shiny app, meaning the user is choosing a different URL end point depending on which zip is chosen.  

We will enable that by having the user choose a different `factors_input` variable, that then gets pasted to the URL for download. We can toggle over to the Shiny app and see how this looks as a user input.

Next we read the csv file using `read_csv()` but first we need to unzip that data with the `unz()` function. 


```{r}
factors_input <- "Global_5_Factors_Daily"

factors_address <- 
  paste("http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/", 
        factors_input, 
        "_CSV.zip", 
        sep="" )

factors_csv_name <- paste(factors_input, ".csv", sep="")

temp <- tempfile()

download.file(
  # location of file to be downloaded
  factors_address,
  # where we want R to store that file
  temp)

Global_5_Factors <- 
  read_csv(unz(temp, factors_csv_name))

head(Global_5_Factors) 
```

Have a quick look and notice that the object is not at all what we were expecting. 

We need to clean up the metadata by skipping a few rows with `skip = 6`. The general lesson here is each time we access data from a new source there can be all sorts of maintenance to be performed. And we need to document it!

```{r}
Global_5_Factors <- 
  read_csv(unz(temp, factors_csv_name), skip = 6 ) 

head(Global_5_Factors)
```

Notice the format of the `X1` column, which is the date. That doesn't look like it will play nicely with our date format for portfolio returns. We can change name of the column with `rename(date = X1)` and clean it up with `ymd(parse_date_time(date, "%Y%m%d"))` from the `lubridate` package.

```{r}
Global_5_Factors <- 
  read_csv(unz(temp, factors_csv_name), skip = 6 ) %>%
  rename(date = X1, MKT = `Mkt-RF`) %>%
  mutate(date = ymd(parse_date_time(date, "%Y%m%d"))) 

head(Global_5_Factors)
```

It looks good, but there's one problem. Fama French have their factors on a different scale from our monthly returns - their daily risk free rate is .03. We need to divide the FF factors by 100. Let's do that with `mutate_if(is.numeric, funs(. / 100))`.

```{r}
Global_5_Factors <- 
  read_csv(unz(temp, factors_csv_name), skip = 6 ) %>%
  rename(date = X1, MKT = `Mkt-RF`) %>%
  mutate(date = ymd(parse_date_time(date, "%Y%m%d")))%>%
  mutate_if(is.numeric, funs(. / 100))

tail(Global_5_Factors)


```

Here we display the end of the Fama French observations and can see that they are not updated daily. That's a cost but it's not limited to publicly available sources. Internal data sources can have a lag in updates as well and we need to be aware of them. 

In general, our Fama French data object looks good and we were perhaps a bit too painstaking about the path from zipped CSV to readable data frame object.  

This particular path can be partly reused for other zipped filed but the more important idea is to document the data provenance that sits behind a Shiny application or any model that might be headed to production.  It is a grind in the beginning but a time saver in the future.  We can toggle over to the Shiny application and see how this is generalized to whatever Fama French series is chosen by the user.   

### To the Analysis

We now have two objects `portfolio_returns_daily` and `Global_5_Factors` and we want to regress a dependent variable from the former on several independent variables from the latter. 

To do that, we can combine them into one object and use `mutate()` to run the model.  It's a two step process to combine them. Let's use `left_join()` to combine them based on the column they have in common, `date`.

Not only will this create a new object for us, it acts as a check that the dates line up exactly because wherever they do not, we will see an NA.

```{r}

portfolio_riingo_joined <- 
  portfolio_riingo %>%
  left_join(Global_5_Factors) %>% 
  mutate(Returns_excess = returns - RF) %>% 
  na.omit()

head(portfolio_riingo_joined)
tail(portfolio_riingo_joined)
```

Notice that the Fama French factors are not current up to today, so we have lots of NAs there. 


We are finally ready for our substance, testing the Fama French factors. Nothing fancy here, we call `do(model = lm(Returns_excess ~ MKT + SMB + HML + RMW + CMA, data = .))` and clean up the results with `tidy(model)`.

```{r}
  
ff_dplyr_byhand <-
  portfolio_riingo_joined %>% 
  do(model = lm(Returns_excess ~ MKT + SMB + HML + RMW + CMA, data = .)) %>% 
  tidy(model)

ff_dplyr_byhand
```

We will display this table in the Shiny app and could probably stop here, but let's also add some visualizations, starting with the rolling R-squared.

We will make use of the `rollify()` function from `tibbletime` to apply our model on a rolling basis. 

First, we choose a rolling window of 100 and then define our rolling function.

```{r}
window <- 100

rolling_lm <- rollify(.f = function(Returns_excess, MKT, SMB, HML, RMW, CMA) {
                              lm(Returns_excess ~ MKT + SMB + HML + RMW + CMA)
                           }, 
                      window = window, 
                      unlist = FALSE)
```

Next we apply that function, which we called `rolling_lm()` to our data frame using `mutate()`.

```{r}
rolling_ff <- 
  portfolio_riingo_joined %>% 
  mutate(rolling_lm = rolling_lm(Returns_excess, MKT, SMB, HML, RMW, CMA)) %>% 
  slice(-1:-window)

tail(rolling_ff %>% select(date, rolling_lm))
```


Notice our object has the model results nested in the `rolling_lm` column. That is substantively fine, but not ideal for creating visualizations on the fly in Shiny. 

First, let's extract the R-squared for this model and plot on a rolling basis.

We will extract that statistic with `glance()` and apply that function to the list column with `map()`.

```{r}
rolling_ff_glance <-
  rolling_ff %>% 
  mutate(glanced = map(rolling_lm, glance)) %>% 
  unnest(glanced) %>% 
  select(date, r.squared)

head(rolling_ff_glance)
```


Then we can visualize with `highcharter` via the `hc_add_series()` function. I prefer to pass an `xts` object to `highcharter` so first we will coerce to `xts`.

```{r}
rolling_r_squared_xts <- 
  rolling_ff_glance %>% 
  tk_xts(date_var = date)


highchart(type = "stock") %>% 
  hc_title(text = "Rolling R Squared") %>%
  hc_add_series(rolling_r_squared_xts, color = "cornflowerblue") %>%
  hc_add_theme(hc_theme_flat()) %>%
  hc_navigator(enabled = FALSE) %>% 
  hc_scrollbar(enabled = FALSE)
```


Now we can port that rolling visualization over to the Shiny app. 

It might also be nice to chart the rolling beta for each factor. Let's invoke `tidy()` from the `broom` package, and then `unnest()`.

```{r}

rolling_ff_tidy <-
  rolling_ff %>%
  mutate(tidied = map(rolling_lm, tidy)) %>% 
  unnest(tidied) %>% 
  select(date, term, estimate, std.error, statistic, p.value)

head(rolling_ff_tidy)

```

We now have the rolling beta estimates for each factor in the `estimate` column. Let's chart with `ggplot()`. We want each `term` to get its own color so we `group_by(term)`, then call `ggplot()` and `geom_line()`.

```{r}
rolling_ff_tidy %>% 
  group_by(term) %>%
  ggplot(aes(x = date, y = estimate, color = term)) + 
  geom_line()
```


Once we carry this `ggplot` chunk over to the Shiny application, our work is done and we can look at the full live execution. 

We have grinded through a lot in this Notebook - data import, wrangling, tidying, regression, rolling regression, and visualization - and in so doing have constructed our Shiny app piece-by-piece.  Our goal was to build a useful tool for visualizing Fama French but also to create code flows that can serve as a template for future work with CSVs, stock prices and modeling. 